{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthikeya-io/image-cartoonization/blob/main/cartoonization_cyclegan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh1DTTiVUdV0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tuhpNuzadEZ",
        "outputId": "97f20b55-15c4-497d-f2ae-4e8f92c30576"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm5E8RTKaeZu",
        "outputId": "5d498c4e-0685-47a8-896b-ccfabce34154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/drive/MyDrive/datasets/DBZ/vegeta': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# !ls \"/content/drive/MyDrive/datasets/DBZ/vegeta\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7E2PdjIUjbq"
      },
      "outputs": [],
      "source": [
        "def load_dataset(dataset_path, image_size, batch_size):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(int(image_size * 1.12), Image.BICUBIC),\n",
        "        transforms.RandomCrop(image_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "\n",
        "    dataset = ImageFolder(dataset_path, transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "    return dataloader\n",
        "\n",
        "anf_dataloader = load_dataset(\"/content/drive/MyDrive/datasets/animeface\", image_size=256, batch_size=1)\n",
        "celeba_dataloader = load_dataset(\"/content/drive/MyDrive/datasets/celeba\", image_size=256, batch_size=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhKs11nMZS3m"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=3),\n",
        "            nn.InstanceNorm2d(in_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=3),\n",
        "            nn.InstanceNorm2d(in_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ciDYJUnZaaC"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_residual_blocks=9):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # Initial convolution block\n",
        "        model = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=7),\n",
        "            nn.InstanceNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "\n",
        "        # Downsampling\n",
        "        in_features = 64\n",
        "        out_features = in_features * 2\n",
        "        for _ in range(2):\n",
        "            model += [\n",
        "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(out_features),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            in_features = out_features\n",
        "            out_features = in_features * 2\n",
        "\n",
        "        # Residual blocks\n",
        "        for _ in range(num_residual_blocks):\n",
        "            model += [ResidualBlock(in_features)]\n",
        "\n",
        "        # Upsampling\n",
        "        out_features = in_features // 2\n",
        "        for _ in range(2):\n",
        "            model += [\n",
        "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "                nn.InstanceNorm2d(out_features),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            in_features = out_features\n",
        "            out_features = in_features // 2\n",
        "\n",
        "        # Output layer\n",
        "        model += [nn.ReflectionPad2d(3), nn.Conv2d(64, out_channels, kernel_size=7), nn.Tanh()]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPsNg8g0Zc1-"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        model = [\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        ]\n",
        "\n",
        "        model += [\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        ]\n",
        "\n",
        "        model += [\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        ]\n",
        "        model += [\n",
        "            nn.Conv2d(256, 512, kernel_size=4, padding=1),\n",
        "            nn.InstanceNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        ]\n",
        "\n",
        "        model += [nn.Conv2d(512, 1, kernel_size=4, padding=1)]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9tf5RQ1ZiIJ"
      },
      "outputs": [],
      "source": [
        "criterion_GAN = nn.MSELoss()\n",
        "criterion_cycle = nn.L1Loss()\n",
        "criterion_identity = nn.L1Loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxBgQI7QZjnN",
        "outputId": "8685018e-1897-498b-80d0-342fdcdcf993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0/100] Batch 0: Loss D_A: 0.7019, Loss D_B: 0.4892, Loss G: 3.9730\n",
            "Epoch [0/100] Batch 100: Loss D_A: 0.2489, Loss D_B: 0.2489, Loss G: 1.2295\n",
            "Epoch [0/100] Batch 200: Loss D_A: 0.2835, Loss D_B: 0.2680, Loss G: 1.3739\n",
            "Epoch [0/100] Batch 300: Loss D_A: 0.2602, Loss D_B: 0.2333, Loss G: 1.4521\n",
            "Epoch [0/100] Batch 400: Loss D_A: 0.2938, Loss D_B: 0.2708, Loss G: 1.4980\n",
            "Epoch [0/100] Batch 500: Loss D_A: 0.3302, Loss D_B: 0.2512, Loss G: 2.0757\n",
            "Epoch [0/100] Batch 600: Loss D_A: 0.1983, Loss D_B: 0.2291, Loss G: 1.7968\n",
            "Epoch [0/100] Batch 700: Loss D_A: 0.3292, Loss D_B: 0.2948, Loss G: 1.3744\n",
            "Epoch [0/100] Batch 800: Loss D_A: 0.2933, Loss D_B: 0.2770, Loss G: 1.2973\n",
            "Epoch [0/100] Batch 900: Loss D_A: 0.1912, Loss D_B: 0.2483, Loss G: 1.5286\n",
            "Epoch [1/100] Batch 0: Loss D_A: 0.2780, Loss D_B: 0.2394, Loss G: 1.2594\n",
            "Epoch [1/100] Batch 100: Loss D_A: 0.2625, Loss D_B: 0.2206, Loss G: 1.4974\n",
            "Epoch [1/100] Batch 200: Loss D_A: 0.1450, Loss D_B: 0.1930, Loss G: 1.2483\n",
            "Epoch [1/100] Batch 300: Loss D_A: 0.2815, Loss D_B: 0.2876, Loss G: 1.2765\n",
            "Epoch [1/100] Batch 400: Loss D_A: 0.2116, Loss D_B: 0.2624, Loss G: 1.2141\n",
            "Epoch [1/100] Batch 500: Loss D_A: 0.2704, Loss D_B: 0.2989, Loss G: 1.8425\n",
            "Epoch [1/100] Batch 600: Loss D_A: 0.1664, Loss D_B: 0.1205, Loss G: 1.6152\n",
            "Epoch [1/100] Batch 700: Loss D_A: 0.3312, Loss D_B: 0.2157, Loss G: 1.3391\n",
            "Epoch [1/100] Batch 800: Loss D_A: 0.2861, Loss D_B: 0.1746, Loss G: 1.5002\n",
            "Epoch [1/100] Batch 900: Loss D_A: 0.2166, Loss D_B: 0.1946, Loss G: 1.6707\n",
            "Epoch [2/100] Batch 0: Loss D_A: 0.3131, Loss D_B: 0.2177, Loss G: 1.8596\n",
            "Epoch [2/100] Batch 100: Loss D_A: 0.2621, Loss D_B: 0.1735, Loss G: 1.1756\n",
            "Epoch [2/100] Batch 200: Loss D_A: 0.1550, Loss D_B: 0.2239, Loss G: 1.3007\n",
            "Epoch [2/100] Batch 300: Loss D_A: 0.2049, Loss D_B: 0.1798, Loss G: 1.8270\n",
            "Epoch [2/100] Batch 400: Loss D_A: 0.3194, Loss D_B: 0.2443, Loss G: 1.4353\n",
            "Epoch [2/100] Batch 500: Loss D_A: 0.2660, Loss D_B: 0.2152, Loss G: 1.5670\n",
            "Epoch [2/100] Batch 600: Loss D_A: 0.2534, Loss D_B: 0.2076, Loss G: 1.2017\n",
            "Epoch [2/100] Batch 700: Loss D_A: 0.2375, Loss D_B: 0.2051, Loss G: 1.8076\n",
            "Epoch [2/100] Batch 800: Loss D_A: 0.2338, Loss D_B: 0.1779, Loss G: 1.2549\n",
            "Epoch [2/100] Batch 900: Loss D_A: 0.3459, Loss D_B: 0.2383, Loss G: 1.2334\n",
            "Epoch [3/100] Batch 0: Loss D_A: 0.1813, Loss D_B: 0.3891, Loss G: 1.1310\n",
            "Epoch [3/100] Batch 100: Loss D_A: 0.2719, Loss D_B: 0.2013, Loss G: 1.1541\n",
            "Epoch [3/100] Batch 200: Loss D_A: 0.2582, Loss D_B: 0.2612, Loss G: 1.2518\n",
            "Epoch [3/100] Batch 300: Loss D_A: 0.2303, Loss D_B: 0.2139, Loss G: 1.0680\n",
            "Epoch [3/100] Batch 400: Loss D_A: 0.3053, Loss D_B: 0.1720, Loss G: 1.1714\n",
            "Epoch [3/100] Batch 500: Loss D_A: 0.2771, Loss D_B: 0.2135, Loss G: 1.6940\n",
            "Epoch [3/100] Batch 600: Loss D_A: 0.2559, Loss D_B: 0.2339, Loss G: 1.5047\n",
            "Epoch [3/100] Batch 700: Loss D_A: 0.2480, Loss D_B: 0.1923, Loss G: 1.7293\n",
            "Epoch [3/100] Batch 800: Loss D_A: 0.2677, Loss D_B: 0.2212, Loss G: 1.2072\n",
            "Epoch [3/100] Batch 900: Loss D_A: 0.2377, Loss D_B: 0.2498, Loss G: 1.2951\n",
            "Epoch [4/100] Batch 0: Loss D_A: 0.1837, Loss D_B: 0.2148, Loss G: 1.2672\n",
            "Epoch [4/100] Batch 100: Loss D_A: 0.2215, Loss D_B: 0.2010, Loss G: 1.3566\n",
            "Epoch [4/100] Batch 200: Loss D_A: 0.2681, Loss D_B: 0.2208, Loss G: 1.5296\n",
            "Epoch [4/100] Batch 300: Loss D_A: 0.2472, Loss D_B: 0.2267, Loss G: 1.2678\n",
            "Epoch [4/100] Batch 400: Loss D_A: 0.1890, Loss D_B: 0.2243, Loss G: 1.4701\n",
            "Epoch [4/100] Batch 500: Loss D_A: 0.1747, Loss D_B: 0.1985, Loss G: 1.3419\n",
            "Epoch [4/100] Batch 600: Loss D_A: 0.2862, Loss D_B: 0.2417, Loss G: 1.2371\n",
            "Epoch [4/100] Batch 700: Loss D_A: 0.2254, Loss D_B: 0.2078, Loss G: 1.1667\n",
            "Epoch [4/100] Batch 800: Loss D_A: 0.3029, Loss D_B: 0.2308, Loss G: 1.5406\n",
            "Epoch [4/100] Batch 900: Loss D_A: 0.2540, Loss D_B: 0.3244, Loss G: 1.3854\n",
            "Epoch [5/100] Batch 0: Loss D_A: 0.2444, Loss D_B: 0.3639, Loss G: 1.1400\n",
            "Epoch [5/100] Batch 100: Loss D_A: 0.2115, Loss D_B: 0.2499, Loss G: 0.9955\n",
            "Epoch [5/100] Batch 200: Loss D_A: 0.2749, Loss D_B: 0.1908, Loss G: 1.3473\n",
            "Epoch [5/100] Batch 300: Loss D_A: 0.2632, Loss D_B: 0.2413, Loss G: 1.1822\n",
            "Epoch [5/100] Batch 400: Loss D_A: 0.2147, Loss D_B: 0.3011, Loss G: 1.1927\n",
            "Epoch [5/100] Batch 500: Loss D_A: 0.2552, Loss D_B: 0.2173, Loss G: 1.2087\n",
            "Epoch [5/100] Batch 600: Loss D_A: 0.1907, Loss D_B: 0.1781, Loss G: 1.1026\n",
            "Epoch [5/100] Batch 700: Loss D_A: 0.2390, Loss D_B: 0.1479, Loss G: 1.1336\n",
            "Epoch [5/100] Batch 800: Loss D_A: 0.2902, Loss D_B: 0.2256, Loss G: 1.2377\n",
            "Epoch [5/100] Batch 900: Loss D_A: 0.2694, Loss D_B: 0.2354, Loss G: 1.1159\n",
            "Epoch [6/100] Batch 0: Loss D_A: 0.2984, Loss D_B: 0.1840, Loss G: 1.1309\n",
            "Epoch [6/100] Batch 100: Loss D_A: 0.2171, Loss D_B: 0.1946, Loss G: 1.2240\n",
            "Epoch [6/100] Batch 200: Loss D_A: 0.2112, Loss D_B: 0.2698, Loss G: 1.1133\n",
            "Epoch [6/100] Batch 300: Loss D_A: 0.2167, Loss D_B: 0.2920, Loss G: 1.0261\n",
            "Epoch [6/100] Batch 400: Loss D_A: 0.2335, Loss D_B: 0.2867, Loss G: 1.3320\n",
            "Epoch [6/100] Batch 500: Loss D_A: 0.1981, Loss D_B: 0.2290, Loss G: 1.0986\n",
            "Epoch [6/100] Batch 600: Loss D_A: 0.3057, Loss D_B: 0.1685, Loss G: 1.0699\n",
            "Epoch [6/100] Batch 700: Loss D_A: 0.2003, Loss D_B: 0.2248, Loss G: 1.3006\n",
            "Epoch [6/100] Batch 800: Loss D_A: 0.1891, Loss D_B: 0.2760, Loss G: 1.2255\n",
            "Epoch [6/100] Batch 900: Loss D_A: 0.2094, Loss D_B: 0.2226, Loss G: 1.0833\n",
            "Epoch [7/100] Batch 0: Loss D_A: 0.2013, Loss D_B: 0.2130, Loss G: 1.0725\n",
            "Epoch [7/100] Batch 100: Loss D_A: 0.2268, Loss D_B: 0.2221, Loss G: 0.9577\n",
            "Epoch [7/100] Batch 200: Loss D_A: 0.2371, Loss D_B: 0.2762, Loss G: 1.7371\n",
            "Epoch [7/100] Batch 300: Loss D_A: 0.2980, Loss D_B: 0.3785, Loss G: 0.9364\n",
            "Epoch [7/100] Batch 400: Loss D_A: 0.2580, Loss D_B: 0.4267, Loss G: 1.1247\n",
            "Epoch [7/100] Batch 500: Loss D_A: 0.2265, Loss D_B: 0.2260, Loss G: 1.3373\n",
            "Epoch [7/100] Batch 600: Loss D_A: 0.1857, Loss D_B: 0.2904, Loss G: 1.2633\n",
            "Epoch [7/100] Batch 700: Loss D_A: 0.3152, Loss D_B: 0.2581, Loss G: 1.1973\n",
            "Epoch [7/100] Batch 800: Loss D_A: 0.2258, Loss D_B: 0.2614, Loss G: 1.0448\n",
            "Epoch [7/100] Batch 900: Loss D_A: 0.3056, Loss D_B: 0.3786, Loss G: 1.1368\n",
            "Epoch [8/100] Batch 0: Loss D_A: 0.2417, Loss D_B: 0.2163, Loss G: 1.0400\n",
            "Epoch [8/100] Batch 100: Loss D_A: 0.2667, Loss D_B: 0.2270, Loss G: 1.2214\n",
            "Epoch [8/100] Batch 200: Loss D_A: 0.2141, Loss D_B: 0.2393, Loss G: 1.1565\n",
            "Epoch [8/100] Batch 300: Loss D_A: 0.2454, Loss D_B: 0.1444, Loss G: 1.0851\n",
            "Epoch [8/100] Batch 400: Loss D_A: 0.2966, Loss D_B: 0.2034, Loss G: 1.4244\n",
            "Epoch [8/100] Batch 500: Loss D_A: 0.2670, Loss D_B: 0.1584, Loss G: 1.1498\n",
            "Epoch [8/100] Batch 600: Loss D_A: 0.1994, Loss D_B: 0.2195, Loss G: 1.3519\n",
            "Epoch [8/100] Batch 700: Loss D_A: 0.2278, Loss D_B: 0.2588, Loss G: 1.1519\n",
            "Epoch [8/100] Batch 800: Loss D_A: 0.2592, Loss D_B: 0.2718, Loss G: 1.5045\n",
            "Epoch [8/100] Batch 900: Loss D_A: 0.2071, Loss D_B: 0.2836, Loss G: 1.0620\n",
            "Epoch [9/100] Batch 0: Loss D_A: 0.3062, Loss D_B: 0.2382, Loss G: 1.5156\n",
            "Epoch [9/100] Batch 100: Loss D_A: 0.2990, Loss D_B: 0.2023, Loss G: 1.2118\n",
            "Epoch [9/100] Batch 200: Loss D_A: 0.2226, Loss D_B: 0.2457, Loss G: 1.2759\n",
            "Epoch [9/100] Batch 300: Loss D_A: 0.2140, Loss D_B: 0.2553, Loss G: 1.0010\n",
            "Epoch [9/100] Batch 400: Loss D_A: 0.2205, Loss D_B: 0.2849, Loss G: 1.1186\n",
            "Epoch [9/100] Batch 500: Loss D_A: 0.2077, Loss D_B: 0.1665, Loss G: 1.3240\n",
            "Epoch [9/100] Batch 600: Loss D_A: 0.1739, Loss D_B: 0.2825, Loss G: 1.5470\n",
            "Epoch [9/100] Batch 700: Loss D_A: 0.2404, Loss D_B: 0.1407, Loss G: 1.2601\n",
            "Epoch [9/100] Batch 800: Loss D_A: 0.2068, Loss D_B: 0.2672, Loss G: 1.1912\n",
            "Epoch [9/100] Batch 900: Loss D_A: 0.2868, Loss D_B: 0.2595, Loss G: 1.0775\n",
            "Epoch [10/100] Batch 0: Loss D_A: 0.2240, Loss D_B: 0.1916, Loss G: 1.2815\n",
            "Epoch [10/100] Batch 100: Loss D_A: 0.2980, Loss D_B: 0.2306, Loss G: 1.5189\n",
            "Epoch [10/100] Batch 200: Loss D_A: 0.2344, Loss D_B: 0.2386, Loss G: 1.1641\n",
            "Epoch [10/100] Batch 300: Loss D_A: 0.2164, Loss D_B: 0.2537, Loss G: 0.9775\n",
            "Epoch [10/100] Batch 400: Loss D_A: 0.2973, Loss D_B: 0.2020, Loss G: 1.1493\n",
            "Epoch [10/100] Batch 500: Loss D_A: 0.2198, Loss D_B: 0.2182, Loss G: 1.2848\n",
            "Epoch [10/100] Batch 600: Loss D_A: 0.2344, Loss D_B: 0.1952, Loss G: 1.2447\n",
            "Epoch [10/100] Batch 700: Loss D_A: 0.1632, Loss D_B: 0.3618, Loss G: 1.8368\n",
            "Epoch [10/100] Batch 800: Loss D_A: 0.3509, Loss D_B: 0.2955, Loss G: 1.1171\n",
            "Epoch [10/100] Batch 900: Loss D_A: 0.1477, Loss D_B: 0.1637, Loss G: 1.4211\n",
            "Epoch [11/100] Batch 0: Loss D_A: 0.1251, Loss D_B: 0.1711, Loss G: 1.3475\n",
            "Epoch [11/100] Batch 100: Loss D_A: 0.2154, Loss D_B: 0.2759, Loss G: 1.2867\n",
            "Epoch [11/100] Batch 200: Loss D_A: 0.1969, Loss D_B: 0.1735, Loss G: 1.2981\n",
            "Epoch [11/100] Batch 300: Loss D_A: 0.3073, Loss D_B: 0.2175, Loss G: 1.1384\n",
            "Epoch [11/100] Batch 400: Loss D_A: 0.2491, Loss D_B: 0.1990, Loss G: 1.1009\n",
            "Epoch [11/100] Batch 500: Loss D_A: 0.2710, Loss D_B: 0.2729, Loss G: 0.8287\n",
            "Epoch [11/100] Batch 600: Loss D_A: 0.2183, Loss D_B: 0.1476, Loss G: 1.2033\n",
            "Epoch [11/100] Batch 700: Loss D_A: 0.2514, Loss D_B: 0.2468, Loss G: 1.3861\n",
            "Epoch [11/100] Batch 800: Loss D_A: 0.2671, Loss D_B: 0.3314, Loss G: 1.3943\n",
            "Epoch [11/100] Batch 900: Loss D_A: 0.3105, Loss D_B: 0.1905, Loss G: 1.0182\n",
            "Epoch [12/100] Batch 0: Loss D_A: 0.2422, Loss D_B: 0.1927, Loss G: 1.2844\n",
            "Epoch [12/100] Batch 100: Loss D_A: 0.2401, Loss D_B: 0.1520, Loss G: 1.5899\n",
            "Epoch [12/100] Batch 200: Loss D_A: 0.1563, Loss D_B: 0.1989, Loss G: 1.2198\n",
            "Epoch [12/100] Batch 300: Loss D_A: 0.2628, Loss D_B: 0.2002, Loss G: 1.4753\n",
            "Epoch [12/100] Batch 400: Loss D_A: 0.2030, Loss D_B: 0.2223, Loss G: 1.4275\n",
            "Epoch [12/100] Batch 500: Loss D_A: 0.2624, Loss D_B: 0.2997, Loss G: 1.2445\n",
            "Epoch [12/100] Batch 600: Loss D_A: 0.2483, Loss D_B: 0.2179, Loss G: 1.0415\n",
            "Epoch [12/100] Batch 700: Loss D_A: 0.2328, Loss D_B: 0.1401, Loss G: 1.0750\n",
            "Epoch [12/100] Batch 800: Loss D_A: 0.1864, Loss D_B: 0.1799, Loss G: 1.4162\n",
            "Epoch [12/100] Batch 900: Loss D_A: 0.2471, Loss D_B: 0.2007, Loss G: 1.3120\n",
            "Epoch [13/100] Batch 0: Loss D_A: 0.2491, Loss D_B: 0.2535, Loss G: 1.2653\n",
            "Epoch [13/100] Batch 100: Loss D_A: 0.1798, Loss D_B: 0.2489, Loss G: 1.0122\n",
            "Epoch [13/100] Batch 200: Loss D_A: 0.2234, Loss D_B: 0.1586, Loss G: 1.2941\n",
            "Epoch [13/100] Batch 300: Loss D_A: 0.2714, Loss D_B: 0.2828, Loss G: 1.0986\n",
            "Epoch [13/100] Batch 400: Loss D_A: 0.2508, Loss D_B: 0.2351, Loss G: 1.2886\n",
            "Epoch [13/100] Batch 500: Loss D_A: 0.2304, Loss D_B: 0.1343, Loss G: 1.2473\n",
            "Epoch [13/100] Batch 600: Loss D_A: 0.2046, Loss D_B: 0.1665, Loss G: 1.3136\n",
            "Epoch [13/100] Batch 700: Loss D_A: 0.2095, Loss D_B: 0.1914, Loss G: 1.1593\n",
            "Epoch [13/100] Batch 800: Loss D_A: 0.2907, Loss D_B: 0.2218, Loss G: 0.9845\n",
            "Epoch [13/100] Batch 900: Loss D_A: 0.2205, Loss D_B: 0.2644, Loss G: 0.9473\n",
            "Epoch [14/100] Batch 0: Loss D_A: 0.2074, Loss D_B: 0.2039, Loss G: 1.0318\n",
            "Epoch [14/100] Batch 100: Loss D_A: 0.2206, Loss D_B: 0.2366, Loss G: 1.2186\n",
            "Epoch [14/100] Batch 200: Loss D_A: 0.1994, Loss D_B: 0.2972, Loss G: 1.0731\n",
            "Epoch [14/100] Batch 300: Loss D_A: 0.2734, Loss D_B: 0.1582, Loss G: 1.0427\n",
            "Epoch [14/100] Batch 400: Loss D_A: 0.1956, Loss D_B: 0.1455, Loss G: 1.2412\n",
            "Epoch [14/100] Batch 500: Loss D_A: 0.2581, Loss D_B: 0.2050, Loss G: 1.3176\n",
            "Epoch [14/100] Batch 600: Loss D_A: 0.3330, Loss D_B: 0.2568, Loss G: 1.0895\n",
            "Epoch [14/100] Batch 700: Loss D_A: 0.2729, Loss D_B: 0.1644, Loss G: 1.3015\n",
            "Epoch [14/100] Batch 800: Loss D_A: 0.2427, Loss D_B: 0.3227, Loss G: 1.0441\n",
            "Epoch [14/100] Batch 900: Loss D_A: 0.2005, Loss D_B: 0.2288, Loss G: 1.2314\n",
            "Epoch [15/100] Batch 0: Loss D_A: 0.2601, Loss D_B: 0.2166, Loss G: 0.9056\n",
            "Epoch [15/100] Batch 100: Loss D_A: 0.2193, Loss D_B: 0.3185, Loss G: 1.0551\n",
            "Epoch [15/100] Batch 200: Loss D_A: 0.2158, Loss D_B: 0.2009, Loss G: 1.0485\n",
            "Epoch [15/100] Batch 300: Loss D_A: 0.2200, Loss D_B: 0.2307, Loss G: 1.1258\n",
            "Epoch [15/100] Batch 400: Loss D_A: 0.2433, Loss D_B: 0.2388, Loss G: 1.0923\n",
            "Epoch [15/100] Batch 500: Loss D_A: 0.2460, Loss D_B: 0.2818, Loss G: 0.9062\n",
            "Epoch [15/100] Batch 600: Loss D_A: 0.2841, Loss D_B: 0.1869, Loss G: 1.1433\n",
            "Epoch [15/100] Batch 700: Loss D_A: 0.2353, Loss D_B: 0.2138, Loss G: 1.0980\n",
            "Epoch [15/100] Batch 800: Loss D_A: 0.2460, Loss D_B: 0.1893, Loss G: 1.2535\n",
            "Epoch [15/100] Batch 900: Loss D_A: 0.1903, Loss D_B: 0.3007, Loss G: 1.1602\n",
            "Epoch [16/100] Batch 0: Loss D_A: 0.2449, Loss D_B: 0.1598, Loss G: 1.1431\n",
            "Epoch [16/100] Batch 100: Loss D_A: 0.2378, Loss D_B: 0.2088, Loss G: 1.1209\n",
            "Epoch [16/100] Batch 200: Loss D_A: 0.1894, Loss D_B: 0.2320, Loss G: 1.1998\n",
            "Epoch [16/100] Batch 300: Loss D_A: 0.2269, Loss D_B: 0.3094, Loss G: 1.2232\n",
            "Epoch [16/100] Batch 400: Loss D_A: 0.2079, Loss D_B: 0.2782, Loss G: 1.0222\n",
            "Epoch [16/100] Batch 500: Loss D_A: 0.2838, Loss D_B: 0.1607, Loss G: 0.9953\n",
            "Epoch [16/100] Batch 600: Loss D_A: 0.2328, Loss D_B: 0.2813, Loss G: 1.1785\n",
            "Epoch [16/100] Batch 700: Loss D_A: 0.2121, Loss D_B: 0.2674, Loss G: 1.4093\n",
            "Epoch [16/100] Batch 800: Loss D_A: 0.1961, Loss D_B: 0.2203, Loss G: 0.9090\n",
            "Epoch [16/100] Batch 900: Loss D_A: 0.2329, Loss D_B: 0.2239, Loss G: 1.0257\n",
            "Epoch [17/100] Batch 0: Loss D_A: 0.2726, Loss D_B: 0.1344, Loss G: 1.2709\n",
            "Epoch [17/100] Batch 100: Loss D_A: 0.2724, Loss D_B: 0.2313, Loss G: 0.8397\n",
            "Epoch [17/100] Batch 200: Loss D_A: 0.1865, Loss D_B: 0.2398, Loss G: 1.2293\n",
            "Epoch [17/100] Batch 300: Loss D_A: 0.1910, Loss D_B: 0.2359, Loss G: 1.1896\n",
            "Epoch [17/100] Batch 400: Loss D_A: 0.2037, Loss D_B: 0.1116, Loss G: 1.1490\n",
            "Epoch [17/100] Batch 500: Loss D_A: 0.2144, Loss D_B: 0.2089, Loss G: 1.1518\n",
            "Epoch [17/100] Batch 600: Loss D_A: 0.2423, Loss D_B: 0.1756, Loss G: 1.1442\n",
            "Epoch [17/100] Batch 700: Loss D_A: 0.2437, Loss D_B: 0.2290, Loss G: 1.1518\n",
            "Epoch [17/100] Batch 800: Loss D_A: 0.2635, Loss D_B: 0.1208, Loss G: 1.3204\n",
            "Epoch [17/100] Batch 900: Loss D_A: 0.2502, Loss D_B: 0.1601, Loss G: 1.2651\n",
            "Epoch [18/100] Batch 0: Loss D_A: 0.1525, Loss D_B: 0.3176, Loss G: 1.2732\n",
            "Epoch [18/100] Batch 100: Loss D_A: 0.2146, Loss D_B: 0.2072, Loss G: 1.0284\n",
            "Epoch [18/100] Batch 200: Loss D_A: 0.2748, Loss D_B: 0.2066, Loss G: 1.2242\n",
            "Epoch [18/100] Batch 300: Loss D_A: 0.2808, Loss D_B: 0.2273, Loss G: 1.0981\n",
            "Epoch [18/100] Batch 400: Loss D_A: 0.1862, Loss D_B: 0.1754, Loss G: 1.0709\n",
            "Epoch [18/100] Batch 500: Loss D_A: 0.2320, Loss D_B: 0.2323, Loss G: 1.1614\n",
            "Epoch [18/100] Batch 600: Loss D_A: 0.2576, Loss D_B: 0.1966, Loss G: 1.1519\n",
            "Epoch [18/100] Batch 700: Loss D_A: 0.2150, Loss D_B: 0.3211, Loss G: 1.5645\n",
            "Epoch [18/100] Batch 800: Loss D_A: 0.2160, Loss D_B: 0.1202, Loss G: 1.2821\n",
            "Epoch [18/100] Batch 900: Loss D_A: 0.2124, Loss D_B: 0.1970, Loss G: 1.1037\n",
            "Epoch [19/100] Batch 0: Loss D_A: 0.2560, Loss D_B: 0.2696, Loss G: 1.1290\n",
            "Epoch [19/100] Batch 100: Loss D_A: 0.2063, Loss D_B: 0.2041, Loss G: 0.9923\n",
            "Epoch [19/100] Batch 200: Loss D_A: 0.1954, Loss D_B: 0.2935, Loss G: 1.4151\n",
            "Epoch [19/100] Batch 300: Loss D_A: 0.2620, Loss D_B: 0.1674, Loss G: 1.1459\n",
            "Epoch [19/100] Batch 400: Loss D_A: 0.2362, Loss D_B: 0.2588, Loss G: 1.2429\n",
            "Epoch [19/100] Batch 500: Loss D_A: 0.2461, Loss D_B: 0.2510, Loss G: 0.9732\n",
            "Epoch [19/100] Batch 600: Loss D_A: 0.2282, Loss D_B: 0.2663, Loss G: 0.9912\n",
            "Epoch [19/100] Batch 700: Loss D_A: 0.2512, Loss D_B: 0.2276, Loss G: 1.1864\n",
            "Epoch [19/100] Batch 800: Loss D_A: 0.2487, Loss D_B: 0.1620, Loss G: 1.2705\n",
            "Epoch [19/100] Batch 900: Loss D_A: 0.2319, Loss D_B: 0.1599, Loss G: 1.0555\n"
          ]
        }
      ],
      "source": [
        "# Initialize the generators and discriminators\n",
        "G_A2B = Generator(3, 3).cuda()\n",
        "G_B2A = Generator(3, 3).cuda()\n",
        "D_A = Discriminator(3).cuda()\n",
        "D_B = Discriminator(3).cuda()\n",
        "\n",
        "# Initialize the optimizers\n",
        "optimizer_G = optim.Adam(list(G_A2B.parameters()) + list(G_B2A.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D_A = optim.Adam(D_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D_B = optim.Adam(D_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (real_A, real_B) in enumerate(zip(celeba_dataloader, anf_dataloader)):\n",
        "\n",
        "        real_A = real_A[0].cuda()\n",
        "        real_B = real_B[0].cuda()\n",
        "\n",
        "        # Update generators\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Identity loss\n",
        "        loss_identity_A = criterion_identity(G_B2A(real_A), real_A)\n",
        "        loss_identity_B = criterion_identity(G_A2B(real_B), real_B)\n",
        "\n",
        "        # GAN loss\n",
        "        fake_A = G_B2A(real_B)\n",
        "        fake_B = G_A2B(real_A)\n",
        "        loss_GAN_A2B = criterion_GAN(D_B(fake_B), torch.ones_like(D_B(fake_B)).cuda())\n",
        "        loss_GAN_B2A = criterion_GAN(D_A(fake_A), torch.ones_like(D_A(fake_A)).cuda())\n",
        "\n",
        "        # Cycle loss\n",
        "        recov_A = G_B2A(fake_B)\n",
        "        recov_B = G_A2B(fake_A)\n",
        "        loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
        "        loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
        "\n",
        "        # Total generator loss\n",
        "        loss_G = (\n",
        "            loss_identity_A + loss_identity_B\n",
        "            + loss_GAN_A2B + loss_GAN_B2A\n",
        "            + loss_cycle_A + loss_cycle_B\n",
        "        )\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # Update discriminators\n",
        "        optimizer_D_A.zero_grad()\n",
        "        optimizer_D_B.zero_grad()\n",
        "\n",
        "        # Discriminator A loss\n",
        "        loss_D_A_real = criterion_GAN(D_A(real_A), torch.ones_like(D_A(real_A)).cuda())\n",
        "        loss_D_A_fake = criterion_GAN(D_A(fake_A.detach()), torch.zeros_like(D_A(fake_A)).cuda())\n",
        "        loss_D_A = (loss_D_A_real + loss_D_A_fake) / 2\n",
        "        loss_D_A.backward()\n",
        "        optimizer_D_A.step()\n",
        "\n",
        "        # Discriminator B loss\n",
        "        loss_D_B_real = criterion_GAN(D_B(real_B), torch.ones_like(D_B(real_B)).cuda())\n",
        "        loss_D_B_fake = criterion_GAN(D_B(fake_B.detach()), torch.zeros_like(D_B(fake_B)).cuda())\n",
        "        loss_D_B = (loss_D_B_real + loss_D_B_fake) / 2\n",
        "        loss_D_B.backward()\n",
        "        optimizer_D_B.step()\n",
        "\n",
        "        # Print progress\n",
        "        if i % 100 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{num_epochs}] Batch {i}: \"\n",
        "                f\"Loss D_A: {loss_D_A.item():.4f}, \"\n",
        "                f\"Loss D_B: {loss_D_B.item():.4f}, \"\n",
        "                f\"Loss G: {loss_G.item():.4f}\"\n",
        "            )\n",
        "\n",
        "    # Save images\n",
        "    os.makedirs(\"/content/drive/MyDrive/cartooncyclegan/output_images\", exist_ok=True)\n",
        "    save_image(fake_A, f\"/content/drive/MyDrive/cartooncyclegan/output_images/fake_A_{epoch}.png\", normalize=True)\n",
        "    save_image(fake_B, f\"/content/drive/MyDrive/cartooncyclegan/output_images/fake_B_{epoch}.png\", normalize=True)\n",
        "    save_image(recov_A, f\"/content/drive/MyDrive/cartooncyclegan/output_images/recov_A_{epoch}.png\", normalize=True)\n",
        "    save_image(recov_B, f\"/content/drive/MyDrive/cartooncyclegan/output_images/recov_B_{epoch}.png\", normalize=True)\n",
        "\n",
        "    # Save model checkpoints\n",
        "    os.makedirs(\"/content/drive/MyDrive/cartooncyclegan/checkpoints\", exist_ok=True)\n",
        "    torch.save(G_A2B.state_dict(), f\"/content/drive/MyDrive/cartooncyclegan/checkpoints/G_A2B_{epoch}.pth\")\n",
        "    torch.save(G_B2A.state_dict(), f\"/content/drive/MyDrive/cartooncyclegan/checkpoints/G_B2A_{epoch}.pth\")\n",
        "    torch.save(D_A.state_dict(), f\"/content/drive/MyDrive/cartooncyclegan/checkpoints/D_A_{epoch}.pth\")\n",
        "    torch.save(D_B.state_dict(), f\"/content/drive/MyDrive/cartooncyclegan/checkpoints/D_B_{epoch}.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY1nXZqNf18o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}